general:
  seed: 42
  train_path: '../preprocessed/train_full.csv'
  test_path: '../preprocessed/test_full.csv'
  n_folds: 5
  test_batch: 128
  column_text_name: 'partial_clean_tweet'
  column_label_name: 'label'
  awp: True
  grid: False
  ensemble: True
  debug: False
  use_subsampling: True
  wandb: False

subsampling:
  train: 200
  test: 200

model:
  name: 'vinai/bertweet-base'
  batch_size: 128
  lr: 2.e-5
  num_epochs: 2
  max_len: 128
  classification_dropout: 0.15
  require_grad: True
  lora: False
  lora_params: 
    rq: 8
    rk: 8
    rv: 8
    rd: 8

adversarial:
  adv_lr: 1.e-5
  adv_eps: 1.e-3
  adv_epoch: 2

grid:
  lr: [2.e-5]
  batch_size: [64]
  num_epochs: [1, 2]

ensemble:
  strategy: 'avg'
  models: [
    {
      name: 'vinai/bertweet-base',
      batch_size: 128,
      lr: 2.e-5,
      num_epochs: 2,
      max_len: 128,
      classification_dropout: 0.15,
      require_grad: False,
      lora: False,
      lora_params: {
        rq: 8,
        rk: 8,
        rv: 8,
        rd: 8,
      }
    },
    {
      name: 'vinai/bertweet-base',
      batch_size: 64,
      lr: 2.e-5,
      num_epochs: 2,
      max_len: 128,
      classification_dropout: 0.15,
      require_grad: False,
      lora: False,
      lora_params: {
        rq: 8,
        rk: 8,
        rv: 8,
        rd: 8,
      }
    },
  ]
  
inference:
  weights: [0.9,0.89,0.91]